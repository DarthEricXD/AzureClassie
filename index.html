<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>实时翻译与说话人识别</title>
    <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-gray-100 flex items-center justify-center h-screen">
    <div class="bg-white p-6 rounded-lg shadow-lg w-full max-w-md">
        <h1 class="text-2xl font-bold mb-4 text-center">实时翻译与说话人识别</h1>
        <div class="mb-4">
            <label class="block text-sm font-medium text-gray-700">选择翻译方向</label>
            <select id="translationDirection" class="mt-1 block w-full p-2 border rounded">
                <option value="en2zh">英译中</option>
                <option value="zh2en">中译英</option>
            </select>
        </div>
        <div class="flex justify-center space-x-4">
            <button id="startBtn" class="bg-blue-500 text-white px-4 py-2 rounded hover:bg-blue-600">开始</button>
            <button id="stopBtn" class="bg-red-500 text-white px-4 py-2 rounded hover:bg-red-600" disabled>停止</button>
        </div>
        <div id="status" class="mt-4 text-sm text-gray-600"></div>
        <div class="mt-4">
            <h2 class="text-lg font-semibold">结果</h2>
            <div id="speaker" class="p-2 bg-gray-50 rounded mb-2">说话人: 未识别</div>
            <div id="original" class="p-2 bg-gray-50 rounded mb-2">原文: 无</div>
            <div id="translation" class="p-2 bg-gray-50 rounded">翻译: 无</div>
        </div>
    </div>

    <script>
        // 配置
        const speechKey = 'YOUR_AZURE_SUBSCRIPTION_KEY'; // 替换为你的 Azure 密钥，或通过后端代理
        const serviceRegion = 'eastus';
        const endpoint = `https://${serviceRegion}.api.cognitive.microsoft.com`;
        const speakerProfiles = {
            'ed1f5481-8b90-4034-81d5-f26ea5db913e': 'Saul Goodman',
            'aa15bd18-c10a-4181-8567-b80c549d4fd8': 'Chuck McGill',
            '1ff0d80b-1687-48b1-a329-003a403a74c7': 'Tuco Salamanca'
        };

        let audioContext, source, processor, stream;
        let isRecording = false;
        let audioChunks = [];
        let lastProcessedTime = 0;
        let config = {};

        // 初始化界面元素
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const status = document.getElementById('status');
        const speaker = document.getElementById('speaker');
        const original = document.getElementById('original');
        const translation = document.getElementById('translation');
        const translationDirection = document.getElementById('translationDirection');

        // 更新配置
        function updateConfig() {
            const direction = translationDirection.value;
            if (direction === 'en2zh') {
                config.srcLang = 'en-US';
                config.tgtLang = 'zh-Hans';
                config.ttsVoice = 'zh-CN-XiaoxiaoNeural';
                config.prompt = '请用英文说话...';
            } else {
                config.srcLang = 'zh-CN';
                config.tgtLang = 'en';
                config.ttsVoice = 'en-US-JennyNeural';
                config.prompt = '请用中文说话...';
            }
        }

        // 显示状态
        function updateStatus(message) {
            status.textContent = message;
        }

        // 更新结果
        function updateResults(speakerText, originalText, translationText) {
            speaker.textContent = `说话人: ${speakerText}`;
            original.textContent = `原文: ${originalText}`;
            translation.textContent = `翻译: ${translationText}`;
        }

        // 开始录音
        async function startRecording() {
            try {
                updateConfig();
                updateStatus(config.prompt);
                stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                audioContext = new AudioContext({ sampleRate: 16000 });
                source = audioContext.createMediaStreamSource(stream);

                // 创建 AudioWorklet 捕获音频
                await audioContext.audioWorklet.addModule(URL.createObjectURL(new Blob([`
                    class AudioProcessor extends AudioWorkletProcessor {
                        constructor() {
                            super();
                            this.buffer = [];
                        }
                        process(inputs) {
                            const input = inputs[0][0];
                            if (input) {
                                this.buffer.push(...input);
                                if (this.buffer.length >= 32000) { // 2秒数据
                                    this.port.postMessage(new Float32Array(this.buffer));
                                    this.buffer = [];
                                }
                            }
                            return true;
                        }
                    }
                    registerProcessor('audio-processor', AudioProcessor);
                `], { type: 'application/javascript' })));

                processor = new AudioWorkletNode(audioContext, 'audio-processor');
                processor.port.onmessage = async (e) => {
                    if (!isRecording) return;
                    const audioData = e.data;
                    audioChunks.push(audioData);
                    if (Date.now() - lastProcessedTime >= 4000) { // 每4秒处理
                        await processAudio();
                        lastProcessedTime = Date.now();
                    }
                };
                source.connect(processor);
                processor.connect(audioContext.destination);

                isRecording = true;
                startBtn.disabled = true;
                stopBtn.disabled = false;
            } catch (err) {
                updateStatus(`错误: ${err.message}`);
            }
        }

        // 停止录音
        function stopRecording() {
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
            }
            if (audioContext) {
                audioContext.close();
            }
            isRecording = false;
            startBtn.disabled = false;
            stopBtn.disabled = true;
            updateStatus('已停止');
            audioChunks = [];
            updateResults('未识别', '无', '无');
        }

        // 处理音频
        async function processAudio() {
            if (!audioChunks.length) return;

            // 转换为 WAV 格式
            const audioData = new Float32Array(audioChunks.flat());
            const wavBlob = await floatToWav(audioData, audioContext.sampleRate);
            audioChunks = [];

            // 语音识别
            const asrStart = performance.now();
            const text = await recognizeSpeech(wavBlob);
            if (!text) return;
            const asrEnd = performance.now();

            // 翻译
            const translationStart = performance.now();
            const translatedText = await translateText(text);
            const translationEnd = performance.now();

            // 说话人识别
            const speakerId = await identifySpeaker(wavBlob);

            // 更新界面
            updateResults(speakerId, text, translatedText);

            // TTS
            const ttsStart = performance.now();
            await synthesizeSpeech(translatedText);
            const ttsEnd = performance.now();

            // 输出时延
            console.log(`ASR时延: ${(asrEnd - asrStart) / 1000}秒`);
            console.log(`翻译时延: ${(translationEnd - asrStart) / 1000}秒`);
            console.log(`TTS时延: ${(ttsEnd - translationEnd) / 1000}秒`);
        }

        // Float32Array 转 WAV
        async function floatToWav(audioData, sampleRate) {
            const buffer = new ArrayBuffer(44 + audioData.length * 2);
            const view = new DataView(buffer);

            function writeString(view, offset, string) {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            }

            const numChannels = 1;
            const bitsPerSample = 16;
            const byteRate = sampleRate * numChannels * bitsPerSample / 8;
            const blockAlign = numChannels * bitsPerSample / 8;

            // WAV 头部
            writeString(view, 0, 'RIFF');
            view.setUint32(4, 36 + audioData.length * 2, true);
            writeString(view, 8, 'WAVE');
            writeString(view, 12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true);
            view.setUint16(22, numChannels, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, byteRate, true);
            view.setUint16(32, blockAlign, true);
            view.setUint16(34, bitsPerSample, true);
            writeString(view, 36, 'data');
            view.setUint32(40, audioData.length * 2, true);

            // 写入 PCM 数据
            for (let i = 0; i < audioData.length; i++) {
                const sample = Math.max(-1, Math.min(1, audioData[i])) * 0x7FFF;
                view.setInt16(44 + i * 2, sample, true);
            }

            return new Blob([buffer], { type: 'audio/wav' });
        }

        // 语音识别
        async function recognizeSpeech(wavBlob) {
            const formData = new FormData();
            formData.append('file', wavBlob, 'audio.wav');
            try {
                const response = await fetch(`${endpoint}/speechtotext/v3.0/transcriptions?language=${config.srcLang}`, {
                    method: 'POST',
                    headers: { 'Ocp-Apim-Subscription-Key': speechKey },
                    body: formData
                });
                const result = await response.json();
                return result.recognizedPhrases?.[0]?.nBest?.[0]?.display || '';
            } catch (err) {
                updateStatus(`语音识别错误: ${err.message}`);
                return '';
            }
        }

        // 翻译
        async function translateText(text) {
            try {
                const response = await fetch(`${endpoint}/translate?api-version=3.0&from=${config.srcLang}&to=${config.tgtLang}`, {
                    method: 'POST',
                    headers: {
                        'Ocp-Apim-Subscription-Key': speechKey,
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify([{ Text: text }])
                });
                const result = await response.json();
                return result[0]?.translations[0]?.text || text;
            } catch (err) {
                updateStatus(`翻译错误: ${err.message}`);
                return text;
            }
        }

        // 说话人识别
        async function identifySpeaker(wavBlob) {
            const formData = new FormData();
            formData.append('file', wavBlob, 'audio.wav');
            const profileIds = Object.keys(speakerProfiles).join(',');
            try {
                const response = await fetch(`${endpoint}/speaker/identification/v2.0/text-independent/profiles/identifySingleSpeaker?profileIds=${profileIds}`, {
                    method: 'POST',
                    headers: { 'Ocp-Apim-Subscription-Key': speechKey },
                    body: formData
                });
                const result = await response.json();
                return speakerProfiles[result.identifiedProfile?.profileId] || 'Unknown_Speaker';
            } catch (err) {
                updateStatus(`说话人识别错误: ${err.message}`);
                return 'Unknown_Speaker';
            }
        }

        // TTS
        async function synthesizeSpeech(text) {
            try {
                const response = await fetch(`${endpoint}/texttospeech/v3.0/synthesize`, {
                    method: 'POST',
                    headers: {
                        'Ocp-Apim-Subscription-Key': speechKey,
                        'Content-Type': 'application/ssml+xml'
                    },
                    body: `<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="${config.tgtLang}">
                        <voice name="${config.ttsVoice}">${text}</voice>
                    </speak>`
                });
                const audioBlob = await response.blob();
                const audioUrl = URL.createObjectURL(audioBlob);
                const audio = new Audio(audioUrl);
                audio.play();
            } catch (err) {
                updateStatus(`TTS错误: ${err.message}`);
            }
        }

        // 事件监听
        startBtn.addEventListener('click', startRecording);
        stopBtn.addEventListener('click', stopRecording);
    </script>
</body>
</html>
